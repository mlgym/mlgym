{"version":3,"file":"static/js/14.e4dd0517.chunk.js","mappings":"mBAGO,MAAMA,EAAcC,OAAOC,OAAO,CACrCC,WAAY,aACZC,cAAe,gBACfC,kBAAmB,oBACnBC,kBAAmB,oBACnBC,kBAAmB,oBACnBC,cAAe,8DCAnB,MAAMC,EAAyD,CAAC,EAI1DC,EAAuF,CACzF,CAACV,EAAYG,YAAa,CAACQ,EAAYC,KACnCC,ECCO,SAA6BC,GAExC,MAAM,eAAEC,EAAgBC,OAAQC,KAAeC,GAASJ,EACxD,MAAO,CAAEG,gBAAeC,EAC5B,CDLwBC,CAAoBR,GAAOC,EAAQZ,EAAYG,WAAW,EAE9E,CAACH,EAAYI,eAAgB,CAACO,EAAYC,IAAgCQ,QAAQC,IAAI,uBACtF,CAACrB,EAAYK,mBAAoB,CAACM,EAAYC,KAC1C,MAAM,cAAEU,EAAa,eAAEC,EAAc,aAAEC,GE8BhC,SAAoCb,GAE/C,MAAM,cAAEW,EAAa,MAAEG,EAAK,cAAEC,EAAa,YAAEC,GAAgBhB,EAEvDY,EAAgC,GAEhCC,EAA8D,CAAC,EAGrE,IAAK,MAAMI,KAAYF,EACnBF,EAAaI,EAASC,MAAQ,IAAMD,EAASE,QAAUF,EAASG,MAChER,EAAeS,KAAK,CAChBC,SAAUL,EAASC,MAAQ,IAAMD,EAASE,OAC1CI,OAAQZ,EACRG,MAAOA,EACPM,MAAOH,EAASG,QAGxB,IAAK,MAAMH,KAAYD,EACnBH,EAAaI,EAASC,MAAQ,IAAMD,EAASO,MAAQP,EAASG,MAC9DR,EAAeS,KAAK,CAChBC,SAAUL,EAASC,MAAQ,IAAMD,EAASO,KAC1CD,OAAQZ,EACRG,MAAOA,EACPM,MAAOH,EAASG,QAIxB,MAAO,CAAET,gBAAeC,iBAAgBC,eAC5C,CF3DgEY,CAA2BzB,GACnFC,EAAOyB,cAAcL,QAAQT,GAC7BV,EAAgB,CAAES,mBAAkBE,GAAgBZ,EAAO,EAE/D,CAACZ,EAAYM,mBAAoB,CAACK,EAAYC,IAAgCQ,QAAQC,IAAI,oBAC1F,CAACrB,EAAYO,mBAAoB,CAACI,EAAYC,KAC1CC,EGVO,SAAoCyB,GAE/C,MAAM,eAAEvB,EAAc,OAAEC,EAAM,OAAEuB,KAAWrB,GAASoB,EACpD,MAAO,CAEHE,aAAcxB,EAEduB,OAAQA,EAAOE,OAEfC,eAAgBxB,EAAKyB,cAAgBzB,EAAK0B,WAC1CC,eAAgB3B,EAAK4B,cAAgB5B,EAAK6B,eACvC7B,EAEX,CHHwB8B,CAA2BrC,GAAOC,EAAQZ,EAAYO,kBAAkB,GAMhG,SAASM,EAAgBF,EAAWC,GAAgD,IAAD,QAAxBqC,EAAW,uDAAG,GAErEhD,OAAOiD,OAAiC,QAA3B,EAACtC,EAAO,EAAAD,EAAKW,sBAAc,QAA1BV,EAAO,GAAwBD,EAAMA,GAE/CsC,GAAOxC,EAAyBwC,KAEpCxC,EAAyBwC,IAAO,EAEhChD,OAAOkD,KAAKvC,EAAOD,EAAKW,gBAAgB8B,QAAQxC,EAAOyC,QAAQC,IAAK1C,EAAOyC,SAC/E,CAGA,SAASE,EAAcC,EAAiCC,GAEpD,IAAK,MAAM9C,KAAQ6C,EAAoB,CAEnC,MAAME,EAAmB/C,EACzB,GAAkC,mBAA9B+C,EAAiBC,SAA+B,CAEhDJ,EAAcG,EAAiB/C,KAAM8C,GACrC,QAIJ,CAEA,MAAM,WAAEG,EAAU,QAAEC,GAAYlD,EAEhCD,EAAkBkD,GAA8CC,EAASJ,EAC7E,CACJ,CAeO,MAAMK,EAA4BN,IAErC,MAAMC,EAA+B,CACjCJ,QAAS,IAAIU,IACb1B,cAAe,IAGnBkB,EAAcC,EAAoBC,GAElCO,YArBJ,SAAqC,GAAyE,IAAzE,QAAEX,EAAO,cAAEhB,KAAkB4B,GAA4B,EAI1F,MAAO,CACHC,aAAcb,EAAQc,KAAO,EAAI,IAAId,QAAWe,EAChDC,UAAWpE,OAAOqE,OAAOL,GACzB5B,cAAeA,EAEvB,CAYgBkC,CAA4Bd,GAAe,EAI9Ce,EAA0BC,IACnCT,YAAY,CAAEhD,OAAQ,CAAEyD,SAAwB,EAWvCC,EAAgCC,IACzCX,YAAY,CAAEhD,OAAQ,CAAE2D,eAA8B,EIlG1D,IAAIC,EAEJ,IAAIC,GAAoB,EACpBC,GAAoB,EAEpBC,EAA4B,EAEhC,MACMC,EAA2B,GA8B3BC,EAAY,KJmD0B,IAA6BC,EAAuBC,EIlD5FP,EAAmBQ,YAAYC,EAA+BC,KJmD9DtB,YAAY,CAAEhD,OAAQ,CAAEuE,mBIlDK,EJkDcL,eAAcC,eIlDvB,EAIhCK,EAAuBC,IACzBT,EAAYhD,KAAKyD,GACbT,EAAYU,QAvCoB,KAwChCC,IAEJZ,IJ6CAf,YAAY,CAAEhD,OAAQ,uBI5CW,EAG/B4E,EAAoC,KACtCd,GAAW,IAAIe,MAAOC,UACtBtB,EAAuBM,EAAWD,EAAS,EAIzCQ,EAAgC,KAClCR,GAAW,IAAIgB,MAAOC,UACtBpB,EAA6BK,EA5DV,GA6DnBA,EAAoB,EACpBgB,WAAWH,EAAmC,IAAOI,KAAKC,SAAS,EAGjEC,EAAO,KACTC,cAAcvB,GACdF,EAA6B,GAC7BF,EAAuB,EAAE,EAGvBmB,EAAuB,KACzBvE,QAAQC,IAAI2D,EAAYU,QACxB5B,EAAyBkB,GACzBA,EAAYU,OAAS,CAAC,EAI1BU,UAAY,IAA6B,IAA5B,KAAEzF,GAAoB,EApEX0F,MAqEpBjF,QAAQC,IAAI,2CAAD,OAA4CV,IArEnC0F,EA0EL,wDAzEfjF,QAAQC,IAAIiF,eAAyBD,GACrCE,MAAMD,eAAyBD,GAC1BG,MAAKC,GAAOA,EAAIC,SAChBF,MAAKE,IACF,MAAMC,EAAiB,YACnB,IAAK,MAAMC,KAAQF,EAAK7E,MAAM,YACpB+E,CAEd,CAJuB,GAMvB3B,IAEA,SAAU4B,IACN,MAAM,MAAEC,EAAK,KAAEC,GAASJ,EAAcK,OAClCD,EACAb,KAEAH,WAAWc,EAAa,IAAMb,KAAKC,UACnCT,EAAoByB,KAAKC,MAAMJ,IAEtC,CARD,EAQI,IAEPK,OAAMC,GAAShG,QAAQgG,MAAMA,IAmDqC,C","sources":["worker_socket/EventsTypes.ts","worker_socket/MainThreadCallbacks.ts","worker_socket/event_handlers/JobStatusHandler.ts","worker_socket/event_handlers/EvaluationResultHandler.ts","worker_socket/event_handlers/ExperimentStatusHandler.ts","worker_socket/WorkerSocket.ts"],"sourcesContent":["// as no more event types are going to be created, it's better to have all of them just here in one place instead of defined in their own files with their handlers\n\n// the name of 'mlgym_event's received on the socket\nexport const MLGYM_EVENT = Object.freeze({\n    JOB_STATUS: \"job_status\",\n    JOB_SCHEDULED: \"job_scheduled\",\n    EVALUATION_RESULT: \"evaluation_result\",\n    EXPERIMENT_CONFIG: \"experiment_config\",\n    EXPERIMENT_STATUS: \"experiment_status\",\n    UNKNOWN_EVENT: \"Unknown event type. No event handler for such event type.\",\n});","import { Row } from \"../redux/table/tableSlice\";\nimport { BufferedDataFromSocket, DataFromSocket, DataToRedux, UpdatesObject } from \"./DataTypes\";\nimport { MLGYM_EVENT } from \"./EventsTypes\";\nimport handleEvaluationResultData from \"./event_handlers/EvaluationResultHandler\";\nimport handleExperimentStatusData from \"./event_handlers/ExperimentStatusHandler\";\nimport handleJobStatusData from \"./event_handlers/JobStatusHandler\";\n\n// ========================= variables ============================//\n\nconst MapEventsFirstOccurrence: { [event: string]: boolean } = {};\n// const TableHeaders: Set<string> = new Set();\n\n// Hashing is faster instead of switching over the the eventType\nconst MapEventToProcess: { [event: string]: (input: JSON, output: UpdatesObject) => void } = {\n    [MLGYM_EVENT.JOB_STATUS]: (data: JSON, update: UpdatesObject): void => {\n        processForTable(handleJobStatusData(data), update, MLGYM_EVENT.JOB_STATUS);\n    },\n    [MLGYM_EVENT.JOB_SCHEDULED]: (data: JSON, update: UpdatesObject): void => console.log(\"Job scheduled found\"),\n    [MLGYM_EVENT.EVALUATION_RESULT]: (data: JSON, update: UpdatesObject): void => {\n        const { experiment_id, charts_updates, table_scores } = handleEvaluationResultData(data);\n        update.chartsUpdates.push(...charts_updates); // process for Charts\n        processForTable({ experiment_id, ...table_scores }, update); // process for Table\n    },\n    [MLGYM_EVENT.EXPERIMENT_CONFIG]: (data: JSON, update: UpdatesObject): void => console.log(\"Exp config found\"),\n    [MLGYM_EVENT.EXPERIMENT_STATUS]: (data: JSON, update: UpdatesObject): void => {\n        processForTable(handleExperimentStatusData(data), update, MLGYM_EVENT.EXPERIMENT_STATUS);\n    },\n};\n\n// ========================= helper methods ============================//\n// prepares the rows and the headers of the table\nfunction processForTable(data: Row, update: UpdatesObject, key: string = \"\"): void {\n    // if defined then merge otherwise take the data as the initial value\n    Object.assign(update[data.experiment_id] ??= data, data);\n    // check if the event_type is provided and if it was the first occurrence? if so return\n    if (key && MapEventsFirstOccurrence[key]) return;\n    // set the flag in order to not form the headers again\n    MapEventsFirstOccurrence[key] = true;\n    // else add every key in the row object to the headers\n    Object.keys(update[data.experiment_id]).forEach(update.headers.add, update.headers);\n}\n\n// loops over buffers, whether created by the websocket or received as a buffered message\nfunction processBuffer(bufferedSocketData: Array<JSON>, updatesHolder: UpdatesObject): void {\n    // loop over all incoming data from socket\n    for (const data of bufferedSocketData) {\n        // assume incoming data type to be BufferedDataFromSocket and verify the event_id\n        const data_from_socket = data as BufferedDataFromSocket;\n        if (data_from_socket.event_id === \"batched_events\") {\n            // if that's the case then do a recursive call on the data and skip afterwards\n            processBuffer(data_from_socket.data, updatesHolder)\n            continue;\n            // NOTE: due to the nature of the incoming messages we know that there is \n            // never a BufferedDataFromSocket inside of another BufferedDataFromSocket\n            // so it's safe to say that this recursion is 1-level-deep only!\n        }\n        // parse data from socket then extract event_type and payload\n        const { event_type, payload } = data as any;\n        // process the payload and load it into the UpdatesObject Object to be later processed into DataToRedux object\n        MapEventToProcess[event_type as keyof typeof MapEventToProcess](payload, updatesHolder);\n    }\n}\n\n// prepare the update to match the redux accepted format \nfunction processUpdatesIntoReduxData({ headers, chartsUpdates, ...experiments }: UpdatesObject): DataToRedux {\n    // TODO: maybe compare the size of the set to its previous size instead of sending every time? \n    // NOTE: currently because of the different headers that are received everytime \n    // because of the MLGYM_EVENT.EVALUATION_RESULT, sending nearly all the time is inevitable \n    return {\n        tableHeaders: headers.size > 0 ? [...headers] : undefined,\n        tableData: Object.values(experiments),\n        chartsUpdates: chartsUpdates,\n    };\n}\n\n// ========================= Callbacks to update the MainThread ============================//\nexport const updateMainThreadCallback = (bufferedSocketData: Array<JSON>) => {\n    // create a place holder for the incoming updates\n    const updatesHolder: UpdatesObject = {\n        headers: new Set(),\n        chartsUpdates: []\n    };\n    // Process the buffer coming from the socket and populate updatesHolder object accordingly\n    processBuffer(bufferedSocketData, updatesHolder);\n    // sending Data to the Main thread to store it in Redux after processing it into the right format\n    postMessage(processUpdatesIntoReduxData(updatesHolder));\n};\n\n// NOTE: no need to buffer these callbacks as we want them to be in real time!!!\nexport const pingMainThreadCallback = (ping: number) => {\n    postMessage({ status: { ping } } as DataToRedux);\n};\n\nexport const connectionMainThreadCallback = (isSocketConnected: boolean, gridSearchId?: string, restApiUrl?: string) => {\n    postMessage({ status: { isSocketConnected, gridSearchId, restApiUrl } } as DataToRedux);\n};\n\nexport const msgCounterIncMainThreadCallback = () => {\n    postMessage({ status: \"msg_count_increment\" } as DataToRedux);\n};\n\nexport const throughputMainThreadCallback = (throughput: number) => {\n    postMessage({ status: { throughput } } as DataToRedux);\n};\n","import { Row } from \"../../redux/table/tableSlice\";\n\ninterface JobStatusPayload extends JSON {\n    \"job_id\": string; // \"2022-11-23--20-08-38-17\",\n    \"job_type\": string; // 1,\n    \"status\": string; // \"RUNNING\",\n    \"grid_search_id\": string; // \"2022-11-23--20-08-38\",\n    \"experiment_id\": number; //  17,\n    \"starting_time\": number; // 1669234123.8701758,\n    \"finishing_time\": number; // -1,\n    \"device\": string; // \"cuda:4\",\n    \"error\": string; // null,\n    \"stacktrace\": string; // null\n}\n\n// transform JSON data into Row (the Job part):\nexport default function handleJobStatusData(jobData: JSON): Row {\n    // remove grid_search_id + key renaming \"status\" to \"job_status\"\n    const { grid_search_id, status: job_status, ...rest } = jobData as JobStatusPayload;\n    return { job_status, ...rest } as Row;\n}","// interface data_evaluation_result {\n//     \"grid_search_id\": \"2022-11-23--20-08-38\",\n//     \"experiment_id\": 18,\n//     \"epoch\": 0,\n//     \"metric_scores\": [{\n//         \"metric\": \"F1_SCORE_macro\",\n//         \"split\": \"train\",\n//         \"score\": 0.04199189495669321\n//     }, {\n//         \"metric\": \"PRECISION_macro\",\n//         \"split\": \"train\",\n//         \"score\": 0.052925666019545944\n//     }, {\n//         \"metric\": \"RECALL_macro\",\n//         \"split\": \"train\",\n//         \"score\": 0.100662497082089\n//     }],\n//     \"loss_scores\": [{\n//         \"loss\": \"cross_entropy_loss\",\n//         \"split\": \"train\",\n//         \"score\": 2.3039234473024095\n//     }]\n// }\n\n// ============================ Input shape ================================\ninterface EvaluationResultPayload extends JSON {\n    epoch: number, // string, // in Graph.tsx parsing: false, \n    grid_search_id: string,\n    experiment_id: number,\n    metric_scores: Array<Score>,\n    loss_scores: Array<Score>\n}\n\ninterface Score {\n    metric?: string,\n    loss?: string,\n    split: string,\n    score: number\n}\n\n// ============================ Output shape ===============================\nexport interface ChartUpdate {\n    chart_id: string,\n    exp_id: number,\n    epoch: number, //string, // in Graph.tsx parsing: false, \n    score: number\n}\n\n// ============================ Main function ==============================\nexport default function handleEvaluationResultData(data: JSON) {\n    // parse the incoming data to EvaluationResultPayload and destruct it\n    const { experiment_id, epoch, metric_scores, loss_scores } = data as EvaluationResultPayload;\n    // to append to experiment values in the charts\n    const charts_updates: ChartUpdate[] = [];\n    // for saving the latest score values to update the table\n    const table_scores: { [latest_split_metric_key: string]: number } = {};\n\n    // loop over the metrics and another over the losses\n    for (const scoreObj of metric_scores) {\n        table_scores[scoreObj.split + \"_\" + scoreObj.metric] = scoreObj.score;\n        charts_updates.push({\n            chart_id: scoreObj.split + \"_\" + scoreObj.metric,\n            exp_id: experiment_id,\n            epoch: epoch,\n            score: scoreObj.score\n        });\n    }\n    for (const scoreObj of loss_scores) {\n        table_scores[scoreObj.split + \"_\" + scoreObj.loss] = scoreObj.score;\n        charts_updates.push({\n            chart_id: scoreObj.split + \"_\" + scoreObj.loss,\n            exp_id: experiment_id,\n            epoch: epoch,\n            score: scoreObj.score\n        });\n    }\n\n    return { experiment_id, charts_updates, table_scores };\n}","import { Row } from \"../../redux/table/tableSlice\";\n\ninterface ExperimentStatusPayload extends JSON {\n    \"grid_search_id\": string;// \"2022-11-23--20-08-38\",\n    \"experiment_id\": number; // 6,\n    \"status\": string; // \"evaluation\",\n    \"num_epochs\": number;// 100,\n    \"current_epoch\": number;// 0,\n    \"splits\": Array<string>; // [\"train\", \"val\", \"test\"],\n    \"current_split\": string;// \"train\",\n    \"num_batches\": number; // 8400,\n    \"current_batch\": number;// 840\n}\n\n// transform JSON data into Row (the Experiment part):\nexport default function handleExperimentStatusData(expData: JSON): Row {\n    // 1. remove grid_search_id\n    const { grid_search_id, status, splits, ...rest } = expData as ExperimentStatusPayload;\n    return {\n        // 2. key renaming \"status\" to \"model_status\"\n        model_status: status,\n        // 3. (extra) progresses calculating & storing them \n        splits: splits.join(),\n        // 4. (extra) turn the split array into string\n        epoch_progress: rest.current_epoch / rest.num_epochs,\n        batch_progress: rest.current_batch / rest.num_batches,\n        ...rest\n    } as Row;\n}","import { connectionMainThreadCallback, msgCounterIncMainThreadCallback, pingMainThreadCallback, throughputMainThreadCallback, updateMainThreadCallback } from './MainThreadCallbacks';\n\n// ========================= variables ============================//\n// Ping to measure Round Trip Time (RTT)\nlet pinging_interval: NodeJS.Timer; // for idealy pinging the server\nconst period: number = 1; // specifying how long the pinging_interval in seconds\nlet lastPing: number = -1;\nlet lastPong: number = -1; // the actual ping is calculated = lastPong - lastPing\n// A counter to measure the throughput\nlet msgCountPerPeriod: number = 0;\n// Buffering Window\nconst BUFFER_WINDOW_LIMIT_IN_MESSAGES = 256;\nconst bufferQueue: Array<JSON> = []; //NOTE: no fear of a race conditions as JS runs on a single thread!\n\n// =~=~=~=~=~=~=~=~=~=~=~=~=~= ~WebSocket~ =~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=//\nconst initSimulation = (filePath: string) => {\n    console.log(process.env.PUBLIC_URL + filePath);\n    fetch(process.env.PUBLIC_URL + filePath)\n        .then(res => res.text())\n        .then(text => {\n            const lineGenerator = (function* () {\n                for (const line of text.split('\\n')) {\n                    yield line;\n                }\n            })();\n\n            onConnect();\n\n            (function logNextLine() {\n                const { value, done } = lineGenerator.next();\n                if (done) {\n                    stop();\n                } else {\n                    setTimeout(logNextLine, 100 * Math.random());\n                    process_mlgym_event(JSON.parse(value));\n                }\n            })();\n        })\n        .catch(error => console.error(error));\n};\n\n// ========================= connection events ============================//\nconst onConnect = () => {\n    pinging_interval = setInterval(send_ping_to_websocket_server, period * 1000);\n    connectionMainThreadCallback(true);\n};\n\n// ========================= data driven events ============================//\nconst process_mlgym_event = (msg: JSON) => {\n    bufferQueue.push(msg);\n    if (bufferQueue.length >= BUFFER_WINDOW_LIMIT_IN_MESSAGES) {\n        flushBufferingWindow();\n    }\n    msgCountPerPeriod++;\n    msgCounterIncMainThreadCallback();\n};\n\nconst onPongReceivedfromWebsocketServer = () => {\n    lastPong = new Date().getTime();\n    pingMainThreadCallback(lastPong - lastPing);\n};\n\n// ========================= helper methods ============================//\nconst send_ping_to_websocket_server = () => {\n    lastPing = new Date().getTime();\n    throughputMainThreadCallback(msgCountPerPeriod / period);\n    msgCountPerPeriod = 0;\n    setTimeout(onPongReceivedfromWebsocketServer, 1000 * Math.random());\n};\n\nconst stop = () => {\n    clearInterval(pinging_interval);\n    throughputMainThreadCallback(0);\n    pingMainThreadCallback(0);\n};\n\nconst flushBufferingWindow = () => {\n    console.log(bufferQueue.length)\n    updateMainThreadCallback(bufferQueue);\n    bufferQueue.length = 0;\n};\n\n// =~=~=~=~=~=~=~=~=~=~=~=~=~= ~WebWorker~ =~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=//\nonmessage = ({ data }: MessageEvent) => {\n    console.log(`>>>>>>>> WebWorker with Dummy WebSocket ${data}`);\n    \n    // if (data === \"CLOSE_SOCKET\")\n    //     stopSimulation();\n    // else if (data.gridSearchId !== undefined && data.socketConnectionUrl !== undefined)\n    initSimulation(\"/event_storage/2024-02-14--22-15-46/event_storage.log\");\n};\n"],"names":["MLGYM_EVENT","Object","freeze","JOB_STATUS","JOB_SCHEDULED","EVALUATION_RESULT","EXPERIMENT_CONFIG","EXPERIMENT_STATUS","UNKNOWN_EVENT","MapEventsFirstOccurrence","MapEventToProcess","data","update","processForTable","jobData","grid_search_id","status","job_status","rest","handleJobStatusData","console","log","experiment_id","charts_updates","table_scores","epoch","metric_scores","loss_scores","scoreObj","split","metric","score","push","chart_id","exp_id","loss","handleEvaluationResultData","chartsUpdates","expData","splits","model_status","join","epoch_progress","current_epoch","num_epochs","batch_progress","current_batch","num_batches","handleExperimentStatusData","key","assign","keys","forEach","headers","add","processBuffer","bufferedSocketData","updatesHolder","data_from_socket","event_id","event_type","payload","updateMainThreadCallback","Set","postMessage","experiments","tableHeaders","size","undefined","tableData","values","processUpdatesIntoReduxData","pingMainThreadCallback","ping","throughputMainThreadCallback","throughput","pinging_interval","lastPing","lastPong","msgCountPerPeriod","bufferQueue","onConnect","gridSearchId","restApiUrl","setInterval","send_ping_to_websocket_server","period","isSocketConnected","process_mlgym_event","msg","length","flushBufferingWindow","onPongReceivedfromWebsocketServer","Date","getTime","setTimeout","Math","random","stop","clearInterval","onmessage","filePath","process","fetch","then","res","text","lineGenerator","line","logNextLine","value","done","next","JSON","parse","catch","error"],"sourceRoot":""}